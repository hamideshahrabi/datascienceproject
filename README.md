# ğŸ§  End-to-End Data Science Project

This repository demonstrates the design and implementation of a complete **Machine Learning pipeline** from data ingestion to model evaluation. It showcases best practices in modular development, configuration-driven workflows, experiment tracking, and pipeline orchestration. 

---

## ğŸš€ Project Overview

This project is structured to represent a real-world production-ready ML system. It uses a modular, scalable, and configurable approach with the help of:

- **YAML-based configuration management**
- **Object-oriented design patterns**
- **MLflow & DagsHub for experiment tracking**
- **Dockerized environment for deployment**

---

## ğŸ§© Key Pipeline Stages

1. **Data Ingestion**
   - Downloads or loads the raw dataset.
   - Stores the data in the artifact folder.

2. **Data Validation**
   - Validates schema and data types.
   - Handles missing values and basic sanity checks.

3. **Data Transformation**
   - Performs preprocessing and feature engineering.
   - Converts raw data into model-ready format.

4. **Model Training**
   - Trains machine learning models using configurable hyperparameters.
   - Stores trained models as binary artifacts.

5. **Model Evaluation**
   - Evaluates model performance using custom metrics.
   - Integrates **MLflow** and **DagsHub** for tracking experiments and visualizing results.

---

## ğŸ”§ Project Workflow

To run or modify the pipeline, follow the steps below:

1. âœ… **Update Configuration Files**  
   - `config.yaml`: Paths and global settings  
   - `schema.yaml`: Data schema definition  
   - `params.yaml`: Model hyperparameters  

2. ğŸ” **Define Entity Classes**  
   - In `src/entity`, define dataclasses for structured config handling.

3. ğŸ§  **Configure with Configuration Manager**  
   - Use `src/config/configuration.py` to manage and return configuration objects.

4. ğŸ§± **Update Pipeline Components**  
   - Modular logic lives in `src/components/`: ingestion, validation, transformation, etc.

5. ğŸ”„ **Update Pipeline Scripts**  
   - Each pipeline (e.g., `data_ingestion_pipeline.py`) glues together the components.

6. â–¶ï¸ **Run `main.py`**  
   - Orchestrates the entire ML workflow end-to-end.

---

## ğŸ—‚ Project Structure

ğŸ“¦ src/
â”œâ”€â”€ config/               # Configuration Manager
â”œâ”€â”€ components/           # Data and Model pipeline steps
â”œâ”€â”€ entity/               # Dataclasses for typed config
â”œâ”€â”€ pipeline/             # Orchestration scripts
â”œâ”€â”€ utils/                # Common utilities
â”œâ”€â”€ logger.py             # Centralized logging
â”œâ”€â”€ constants.py          # Constant values



---

## ğŸ“¦ Tools and Technologies

- **Python 3.9**
- **scikit-learn, pandas, numpy**
- **MLflow**
- **DagsHub**
- **PyYAML, Box**
- **joblib**
- **Docker** (optional for deployment)

---

## ğŸ“‹ Future Improvements

- Add unit tests
- Add Docker + CI/CD (GitHub Actions)
- Add Streamlit web app for model inference

---

## ğŸ¤ Contribution

Contributions are welcome! Please feel free to submit issues or pull requests.

---

